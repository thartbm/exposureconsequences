---
title: "Motor Learning Without Moving"
subtitle: "Figures and Statistics"
author:
- Ahmed A. Mostafa
- Bernard Marius 't Hart
- Denise Henriques
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

```{r setup, cache=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(comment='')
```

# Overview

This document collects statistics and figures to be used in the manuscript on the exposure consequences project.

## Figures

These are the planned figures. Figures one and two will not be included here as they are not generated from data but illustrate the setup, tasks and experimental design.

1. Setup & Task (goal: visualize the experiment)
    + A: Setup (side view)
    + B: Rotated training (top view)
    + C: No-cursor reaches (top view)
    + D: Localization (top view)
2. Experimental Design
    + A: Task order (instead of the table in the previous paper)
    + B: Main localization measures
3. Localization
    + A: Exposure, active and passive (goal: small/no difference between active and passive)
    + B: Classic, active and passive (goal: larger difference between active and passive, different generalization?)
4. Reach Aftereffects (goal: show motor changes are there, are robust and compare to classic)
    + A: Exposure reach aftereffects (initial & later)
    + B: Classic & Exposure reach aftereffects (initial only? or all?)
5. Generalization Maxima
    + A: Maxima of active localization after classic and exposure training
    + B: Maxima of reach aftereffects after classic and exposure training

Maybe:

6. Correlation
    + A: Exposure, active and passive localization predicting reach aftereffects
    + B: Classic, active and passive localization predicting reach aftereffects
7. Individual localization responses

## Source scripts

The scripts doing the statistics and creating figures are in a few separate R files. They allow many options, but here we will only see / do the ones that we think should go to the manuscript.

We load those other scripts:

```{r loadsources}
source('shared.R') # functions used everywhere
source('localization.R') # functions for localization data
source('nocursor.R') # functions for no-cursor reach data
source('relateLocalizationNoCursors.R') # functions that correlate the two kinds of data
```

## Packages

We try to import some packages used here. This has already been attempted by `shared.R` (for people without R Studio), but in the chunk below we can easily take action if the required packages are not available on the current system.

```{r require_lme_packages}
required.packages = c('lme4', 'lmerTest')
installRequire.Packages(required.packages, installmissing=FALSE)
if (!all(required.packages %in% installed.packages())) {
  warning('packages required for analysis are not installed, later chunks will fail\n')
}
```

**If this fails: set the `installmissing` argument to TRUE to install the packages.** 

Note: the packages `foreach` and `doParallel` will speed up the bootstrapping of peak fits, but the scripts will also run without.

Note: the package `svglite` is used to generate SVG files with figures, but the scripts will simply skip this if the package is not available.

## Topics

In the manuscript we'll first show that there indeed are localization shifts after exposure training (with the no-cursor data) and then compare that with the classic. Second are the no-cursor reaches (and the reach aftereffects). Then we'll look at generalization patterns of localization shifts and reach aftereffects after both classic and exposure training.

A further topic we only explore in this notebook is if localization shifts can predict no-cursor changes:

1. Analyses (some general remarks about analyses)
2. Participants (participant demographics)
3. Localization
4. Reach aftereffects
5. Generalization Maxima
6. Correlations

## Analyses

### ANOVA or LME

Since in the localization tasks there is missing data at some points (usually at the extremes) we can't use ANOVAs directly on the data. Imputation is going to be very hard because the missing data is systematically missing, and there is no (good) model for the localization data (for reach aftereffects it might work). An alternative to imputation is to either remove participants or removed measurement locations, but that would mean deleting 7/21 participants or 4/7 locations, so at least 1/3 of the data, which would mean a big loss in power. As the data is systematically missing, this might also skew results away from what would be observed in the general population.

Hence, we are using Linear Mixed Effects models as an alternative, but the output of those models are not readily interpretable by everybody. Initially we decided to use the $\chi^2$ approach (from the `Anova` function from the `car` package). However, a paper from Steven G. Luke (2017, _Evaluating significance in linear mixed-effects models in R_) suggests using Satterthwaite approximation. So all functions running an LME now take an argument that tells them to use the Satterthwaite method, but it can be set to chi-squared as well. It turns out that it doesn't matter much, so it's a good sanity check. For consistency across all the analyses, we set the value of that argument here:

```{r set_LMEmethod}
LMEmethod <- 'Satterthwaite'
# options: 'Satterthwaite', 'chi-squared'
```

If `LMEmethod` is set as 'chi-squared', we need some other packages to be able to run this:

```{r requireChiSquared_packages}
if (LMEmethod=='chi-squared') {
  required.packages = c('nlme', 'car')
  installRequire.Packages(required.packages, installmissing=FALSE)
  # we check if that worked and otherwise
  if (!all(required.packages %in% installed.packages())) {
    LMEmethod <- 'Satterthwaite'
  }
}
```

If the packages were not installed (we won't force you), we do switch back to the Satterthwaite method so that we don't generate errors instead of statistical results:

LME analyses have fixed effects, usually the effects we are interested in, as well as random effects, which are usually not the effects we are interested in. Fixed effects are comparable to 'factors' in ANOVA's, and including random effects is conceptually (but not mathematically) comparable to making it a repeated measures design, or including covariates. We'll spell out the fixed and random effects included in each analyses right before showing the results, so that those results can easily be put in the proper context by the reader, and hence not all details are mentioned in the methods section.

### Generalization Maxima

Since participants trained with a single target, we could look at generalization curves. These appear to be shifted more counter clockwise after exposure training compared to classic, so we want to quantify this. But, for most of the curves, and especially the ones that seems to be shifted, we don't have data on the full curve on one side of the peak, which limits the interpretability of the analysis somewhat. We can still bootstrap the __peak__ of Guassian functions fit to the data across many samples of participants for the exposure group and for the delayed localization after classic training, but not for the online localization after classic training (the shifts are even larger there...). This data is presented in the paper, where we only look at the peak of the localization -- not the width of the generalization curve.

The range of the data doesn't include the peak in some participants, which makes it hard to fit a normal function to every individual participant's data. In one alternative approach, we assumed that the minimal change in the data, occurs far beyond the boundaries of the measured workspace, and converted the data to a cumulative distribution, assuming that at some point it would revert back to the base level we see on the other side of the peak. We can scale back to a range from 0 to 1, to allow fitting a _cumulative_ normal function where the 50% point corresponds to the peak of the generalization curve. This method is more stable, and seems to works fairly well on the datasets where the more common fits also work (but we didn't test online localization from the classic experiment). Code for this approach is available (functions `psychometricGeneralization()`, `pnormMSE()` and `getCDFpeaks()` at the end of the file: `shared.R`), but *these analysis are not presented* in the paper or notebook, but the code is left here for the interested reader.

# Participants

Here are some simple descriptives of participants, as well as the selection of participants based on how they performed in detecting the very brief blink of the cursor (2 frames ~33 ms) during the exposure training. This ensures that the participants paid attention to the cursor during training, which is probably required for the cross-sensory error signal to have the desired effect of proprioceptive recalibration.

## Performance

Let's look at the blink detect performance.

```{r plotBlinkDetectionPerformance}
plotBlinkDetection()
```

Turns out the blink detection is hard and needs practice. But three participants are still performing around chance in the rotated training task, so we want to remove them from the data:

```{r selectHighPerformingParticipants}
blinks <- load.DownloadDataframe(informationURLs['blinkdetect'],'blinkdetect_exposure.csv')
OKparticipants <- blinks$participant[which(blinks$rotated_b == 1 & blinks$performance > 0.65)]
```

We'll also set a variable that is to be passed to all functions to exclude the underperformers:

```{r set_selectPerformance_variable}
selectPerformance <- TRUE
```

Which can also be set to FALSE to include those three participants in all analyses and plots.

## Demographics

First load the data, and select only the well performing ones:

```{r load_select_participantInfo}
participants <- load.DownloadDataframe(informationURLs['demographics'],
                                       'participants_exposure.csv')

if (selectPerformance) {
  participants <- participants[which(participants$participant %in% OKparticipants),]
}
```

We want to know some demographics, first mean age, and standard deviation:

```{r age_stats}
kable( c('mean'=mean(participants$age), 'sd'=sd(participants$age)),
       col.names=c('age'),
       caption='age of participants')
```

And the distribution of sex:

```{r sex_distribution}
kable( table(participants$sex),
       caption='sex of participants',
       col.names=c('sex','frequency'))
```

## Localization selection statistics

We've manually selected localization responses that were unlikely to be true responses. Here are the statistics on how many were removed in each dataset. For exposure:

```{r exposure_loc_selection}
kable( aggregate(trials ~ rotated * passive, 
                 data=countSelectedLocalizations('exposure'), 
                 FUN=mean), 
       caption='percentages selected localization trials after exposure training')
```

For classic:

```{r classic_loc_selection}
kable( aggregate(trials ~ rotated * passive,
                 data=countSelectedLocalizations('classic'), 
                 FUN=mean), 
      caption='percentages selected localization trials after classic training')
```

This is highly similar between conditions and training paradigms, and at an acceptable level.

We decided not to use any more outlier removal, but it is implemented and can be switched on in the code: set the `removeOutliers` argument to the `getPointLocalization` function in `shared.R` to TRUE.

## No-cursor selection statistics

Similarly, we get the percentage of selected trials in no-cursor reach tasks in exposure training:


```{r exposure_nocursor_selection}
kable( aggregate(trials ~ 1, 
                 data=countSelectedNoCursors('exposure'), 
                 FUN=mean), 
       caption='percentages selected no-cursor trials after exposure training')
```

And in the classic training group:

```{r classic_nocursor_selection}
kable( aggregate(trials ~ 1, 
                 data=countSelectedNoCursors('classic'), 
                 FUN=mean), 
       caption='percentages selected no-cursor trials after classic training')
```

These high percentages may indicate there are some additional no-cursor reaches that could be considered outliers and romeving them would not constitute a big loss of data. However, as it is, the variance in the dataset appears to mostly reflect inidividual differences and not noise, so we kept the data as is.

# Localization

Here is a plot, probably figure 2:

```{r fig3_localization, fig.width=7.5, fig.height=3}
plotLocalization(selectPerformance=selectPerformance,remove15=FALSE)
```

It seems that just like after classic training, there is a change in localization responses following exposure training. There might still be a difference between active and passive localization after exposure training - contrary to our expectations - but it could be smaller than the difference after classic training. This will need to be tested.

## Missing data

Here are the counts of participants with valid localization estimates in all 4 tasks for every hand angle. Missing data occurs here, because we don't extrapolate beyond the range of the data.

```{r localization_count_table}
kable(getLocCountTable(selectPerformance=selectPerformance),
      caption='count of participants with localization data at each hand angle')
```

Since there is a good amount of missing data at the 15 degree location, we exclude that angle from analysis, with a switch in analysis functions `remove15=TRUE`, which can be set to false if we'd want.

## Is there a localization shift after exposure training?

We first want to see if there is any overall effect of aligned versus rotated training in the exposure group (i.e.: are there any localization shifts at all?), with movement type and hand angle added as fixed effects as well, and particiant as random effect.

```{r loc_exp}
exposureLocalization(remove15=TRUE, 
                     LMEmethod=LMEmethod, 
                     selectPerformance=selectPerformance)
```

Wether or not the feedback was rotated makes a difference according to two terms in the model. In other words: exposure training caused localization responses to shift (systematically), as we could already guess by looking at the figure. This means we can look at the difference between repsonses after rotated and after aligned feedback: the training induced shift in localization. The shifts are also different, given different hand angles, which means there is some form of generalization pattern.

## Effects of movement type and hand angle in exposure localization

Let's first see if there is a difference between active and passive localization after exposure training.

```{r loc_exp_movtype}
exposureLocalizationShift(remove15=TRUE, 
                          LMEmethod=LMEmethod, 
                          selectPerformance=selectPerformance)
```

There is no difference between passive and active localization after exposure training, even with double the number of trials as in classic delayed.

## Comparing localization after exposure and classic training

Let's see what we get in a model with all data (localization differences between rotated and aligned though) from both groups:

```{r loc_ombnibus}
groupLocalization(model='full', 
                  remove15=TRUE, 
                  LMEmethod=LMEmethod, 
                  selectPerformance=selectPerformance)
```

In the exposure group there is no effect of movement type, but we see one here. This could be because the effect of movement type is much larger in the classic group. This would predict an interaction between group and movement type, and we do see that (p=.035). (Including the three)

Perhaps hand angle factored into this somehow, so let's see what happens if we remove it:

```{r loc_groups_movementtype}
groupLocalization(model='movementtype', 
                  remove15=TRUE, 
                  LMEmethod=LMEmethod, 
                  selectPerformance=selectPerformance)
```

Still an effect...

Let's see if the classic group still shows an effect of movement type (with the current preprocessing procedures):

```{r loc_cla}
classicLocalizationShift(remove15=TRUE, 
                         LMEmethod=LMEmethod)
```

So there is a reliable effect of movement type in the classic group, which is in line with the interaction between group and movement type.

Also note the absence of an effect of hand angle in the classic group here -- probably because we excluded the 15 degree target, but it does appear that after classic training the generalization curve for delayed localization is much flatter.

To sum up our findings on localization:

1. Rotated exposure training leads to changes in localization responses.

2. In the exposure group, we find no difference between localization responses with and without efference copies.
3. In the classic group, the localization responses informed by efference copies, are shifted more than those with only proprioception (as found before).
4. This predicts an interaction between movement type and group in the combined analysis, and we see that.

5. The shift in localization is different across the workspace in the exposure group. I.e., there is a generalization _curve_.
6. This effect is not present in the classic group, so that generalization can be said to be flat; there is no _curve_. **NOTE:** When we do include the 15 degree target, there is an effect of hand angle though.


# Reach Aftereffects

Here is a plot, probably Figure 4:

```{r fig4_reachaftereffect, fig.width=7.5, fig.height=3}
plotReachAftereffects(selectPerformance=selectPerformance)
```

Messages of the figure:

- Panel A: there are substantial and persisting reach aftereffects
- Panel B: that are somewhat lower than those in classic (but not that much? but only for some targets?)

These claims require analyses.

## Is there an effect of exposure training on reach aftereffects?

First, we look at the no-cursor reaches after exposure training only (see Fig 3a). We show that no-cursor reaches change direction immediately after exposure training (iteration 1):

```{r RAE_exposure,include=TRUE}
exposureNoCursorChange(LMEmethod=LMEmethod, 
                       selectPerformance=selectPerformance)
```

So this shows that whether or not the feedback was rotated mattered for the reach deviations in the exposure training group, i.e.: there are reach aftereffects.

## Do the motor changes persist during localization?

The second analysis is to show that the reach aftereffects aren't all that different for first and later iterations of the rotated no-cursor tasks. (The rotated data is always compared with _all_ aligned iterations of the task.)

```{r RAE_exp_iterations}
exposureAftereffectsPersistent(LMEmethod=LMEmethod, 
                               selectPerformance=selectPerformance)
```

There doesn't seem to be an effect of iteration, and no interaction with target. Also, looking at the figure, there would not be decay, as the reach aftereffects seem to get larger. We continue assuming there is no substantial change in the reach aftereffects between the first iteration and the average across the other four iterations. We now also see an effect of target angle: the generalization curve is not flat.

## Are reach aftereffects comparable between classic and exposure training?

Finally, we want to see if there is a difference between the reach aftereffects observed after exposure training and those after classic training. Here we combine the data across all iterations (see Fig 3b).

```{r RAE_exp_cla}
exposureClassicReachAftereffects(LMEmethod=LMEmethod, 
                                 selectPerformance=selectPerformance)
```

So there is no effect of training (overall reach aftereffect magnitudes are not very different between classic and exposure training), but there is an effect of target (there is a generalization curve, as we can clearly see in the data) and there is an interaction between training and target (the generalization curves are different between classic and exposure training). This last bit is very similar to the pattern observed in localization, but we'll get back to that later.

The figure looked like there might be some difference between the two training types, mainly in where the generalization curves peak. There is no main effect of training type, so the overall magnitude of reach aftereffects is not different.

However, the main take-away message from this analysis is that the motor changes measured with changes in open-loop reach direction are of similar magnitude in both types of training. That is, passive exposure training (proprioceptive recalibration) drives motor changes.

To sum up:

1. Rotated exposure training evokes substantial shifts in no-cursor reach direction across the workspace,
2. these changes persist over time, and
3. these changes are comparable to the ones we found after classic training, and
4. there is some generalization curve across the workspace, but

# Generalization Maxima

Here we'll look at the differences in generalization suggested by both the LME on localization shift and on reach aftereffects. In particular, we estimate the peak of each curve by bootstrapping participants and fitting a Gaussian (normal) function a 1000 times. This disregards other possible differences in generalization curves: the width of the curve, the peak amplitude or the offset. There may be effects there, but we are most interested in the location of the maxima.

First, a figure:

```{r fig5_generalization_maxima, fig.width=7.5, fig.height=3}
plotGeneralization(generateSVG=FALSE,
                   selectPerformance=selectPerformance)
```


## Localization

There was an interaction between group and hand angle on localization shifts in the LME (far) above, which suggests different generalization curves in the two groups. Here are the confidence intervals for the peaks of the generalization curves for _active_ localization:

```{r localization_peaks}
getPeakLocConfidenceInterval(group='classic',
                             CIs=c(.95), 
                             movementtype='active', 
                             LRpart='all',
                             selectPerformance=FALSE)
getPeakLocConfidenceInterval(group='exposure',
                             CIs=c(.95), 
                             movementtype='active', 
                             LRpart='all', 
                             selectPerformance=selectPerformance)
```

The median values are a little over 13 degrees apart, and median for classic falls just outside the exposure confidence interval. This could explain the interaction between group and hand angle on localization shifts, but the other way around it's not so clear.

## Reach aftereffects

Now we look at the confidence intervals for the maxima of the generalization curves of reach aftereffects after both exposure training and after classic training:

```{r RAE_peaks}
getPeakConfidenceInterval('classic', 
                          part='all', 
                          CIs=c(.95), 
                          selectPerformance=selectPerformance)
getPeakConfidenceInterval('exposure', 
                          part='all', 
                          CIs=c(.95), 
                          selectPerformance=selectPerformance)
```

The 95% confidence interval for the classically trained group includes the 45 degree target (where the visual feedback was during training) but not the 75 degree target, whereas the 95% confidence interval for the exposure training group includes the 75 degree target (where the hand really was during training) but not the 45 degree target. Something might be going on -- but the two confidence intervals are not different from each other, which is not in line with the LME results. We can't conclude much, except that we don't know if the differences in generalization curves comes from a difference in the location of the maxima, or perhaps the width of the curves, or both.



# Correlation

In this section we look at how recalibrated proprioception (hand localization) correlates with reach aftereffects.

This plot shows regression lines with localization shifts as predictor of reach aftereffects, potentially Figure 4:

```{r scattergrams, fig.width=7.5, fig.height=2.5}
correlations <- correlateNoCursorsLocalization(selectPerformance=selectPerformance)
```

It looks like for exposure, both types of localization might be able to predict of reach aftereffects, whereas in classic it is only passive localization. But are any of these reliable? The function also returns the correlation tests and linear regressions for all four datasets.

## Pearson Correlations

As one possibility, we can look at the correlations:

```{r pearson_rhos}
for (name in names(correlations)) {
  cat(name)
  print(correlations[[name]][['cortest']])
}
```

None are significant... some are "trending".

## Linear regression models

And those linear regression models?

```{r linreg, eval=FALSE, include=FALSE}
# this code won't run or show up in output, just included here if you want to play with it:
for (name in names(correlations)) {
  cat(name)
  print(correlations[[name]][['linearm']])
}
```

[code/results hidden] They get the same p-value, so that seems fine.

## Multiple regression models

We've decided to use all the no-cursor blocks for these regression and correlations, so perhaps we shouldn't include the one below where I only use the initial block. The idea behind using only the initial block is that that initial block matches the time the localization is done -- relative to the last training.

Here I do step-wise multiple regression, allowing active and passive localization shift as predictor of reach aftereffects.


```{r step_exposure}
multipleRegressionLocalization(group='exposure', 
                               NCpart='all', 
                               selectPerformance=selectPerformance)
```

The algorithm decides to retain active localization as predictor, but the regression model isn't very good. The problem could be that we need no-cursor data and localization data that are both recorded immediately after training. This also makes it more comparable to the classic group that had fewer blocks of data.

But if we look at only the initial no-cursor block, the model doesn't change much. It still uses only active localization and the p-value goes down a bit and the R-squared goes up a bit. For the classic group...

converges on using only an intercept term for the classic group. This might mean that in te classic group there is no relationship between reach aftereffects and localization, or this might mean that in the exposure group we have sufficient amount of measurements to detect the relationship, as we have twice the number of localization trials. One way to rule this out is to use only the first half of the localization data from the exposure group.

Here is what the step-wise multiple regressions look like with all available data:

```{r step_exposure_initial}
multipleRegressionLocalization(group='exposure',
                               NCpart='initial',
                               LRpart='both', 
                               selectPerformance=selectPerformance)
```

For classic training, it looks like this:

```{r step_classic_initial}
multipleRegressionLocalization(group='classic',
                               NCpart='initial',
                               LRpart='both')
```

The step-wise multiple regression algorithm thinks an intercept-only model is best. Now is this because localization after classic training has little to no information on reach aftereffects, or is this because we have double the data in the exposure group? We can test this partially by throwing away the second half of the data for the exposure group:


```{r step_exposure_firsthalf}
multipleRegressionLocalization(group='exposure',
                               NCpart='initial', 
                               LRpart='first', 
                               selectPerformance=selectPerformance)
```

Now we have an intercept model for the exposure group too. Perhaps this means that previous papers didn't find any relationships between recalibrated proprioception and reach aftereffects because not enough data was collected to overcome the noisiness of the measurements?

Maybe including more data for the model for classic might work better:


```{r step_classic_all}
multipleRegressionLocalization(group='classic',
                               NCpart='all',
                               LRpart='both')
```

And now we get the passive localization as in the initial regressions.

It also makes me wonder if reach aftereffects in classic can be better predicted by the online localization responses:


```{r step_online}
multipleRegressionLocalization(group='online',
                               NCpart='all')
```

And that might be the case as one term is preserved by the model, but on the other hand, the predictions are pretty bad.

What do these correlations mean? I'm not sure. There seems to be some relationship but it's not very strong. So either there are other factors playing a role, the measurements are too noisy, or we don't have enough power to detect any relationships. All of these are very likely, and then there is the other alternative that there is no relationship. So it's probably better not to include these (unplanned) analyses in the manuscript and leave this for a better dataset.

# Conclusions

What can we conclude? Here is an initial list of what we might conclude.

1. **Exposure training leads to shifts in localization**, that are _indistinguishable_ for active and passive localization. Since both active and passive movements access recalibrated proprioception, while active localization also accesses updated predictions, this indicates that: _hand localization relies on (recalibrated) proprioception_.

    1. Additionally, since predictions should not be updated in exposure training, active localization could have resulted in a _lower_ shift in localization if hand location were estimated by combining recalibrated proprioception and updated prediction in a maximum likelihood way. If anything however, active localization is slightly larger (non-significant) than passive localization. This indicates a non-Bayesian process of combining predictions and perceptions -- in this case.

    2. After classic training (2016 paper) there was an effect of movement type, which indicates that with a training regime that allows updates of predicted sensory consequences, these updates do indeed also affect hand localization.

    3. Within the workspace, there is an interaction between movement type and training group, which is predicted by there being an effect of updated predictions on active localization after classic training, but not after exposure training.
    
    4. The generalization curves for localization are different between the two groups.

2. **Exposure training leads to robust reach aftereffects** that are _indistinguishable_ in magnitude to those after classic training (although they seem to have a different generalization curve). This means that evoking propceptive recalibration with visual-proprioceptive discrepancies leads to motor changes. In other words: _recalibrated proprioception informs motor control_.

    1. The generalization curves of reach aftereffects look somewhat different for both groups, which can be explained by the effect of updated predictions that factors into the curve for the classic training group, but not the exposure training group. However, it's not a large difference, and the analyses are ambiguous. Either way, we find no (statistically reliable) evidence that the updated predictions matter much for the generalization curves, which can have several causes all outside the scope of this paper.


```{r create_svgs, cache=FALSE, include=FALSE}
# This chunk creates SVG files but shouldn't show up in the PDF / HTML 
plotLocalization(classicOnline=FALSE,
                 generateSVG=TRUE, 
                 selectPerformance=selectPerformance)
plotReachAftereffects(generateSVG=TRUE,
                      selectPerformance=selectPerformance)
plotGeneralization(generateSVG=TRUE,
                   selectPerformance=selectPerformance)
correlations <- correlateNoCursorsLocalization(generateSVG=TRUE, 
                                               selectPerformance=selectPerformance)
```

